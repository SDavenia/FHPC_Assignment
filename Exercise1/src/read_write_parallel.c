#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <getopt.h>
#include <time.h>
#include <mpi.h>
#include <omp.h>

#include "read_write_parallel.h"


void write_pgm_parallel( unsigned char *ptr, int maxval, int xsize, int ysize, const char *fname, int rank, int size, int rows_initialize){
  /*
  INPUT:
    - ptr: pointer to the memory location where the matrix is stored.
    - maxval: 255
    - xsize, ysize: size of the pgm image, in our case it is kxk
    - fname: name of the file where the pgm image is stored.
    - rank, size: MPI quantities
    - rows_initialize: how many rows each MPI process should write
  */
  
  MPI_File fh;
  MPI_Offset disp;

  MPI_File_delete(fname, MPI_INFO_NULL);
  MPI_File_open(  MPI_COMM_WORLD, fname, 
                MPI_MODE_CREATE | MPI_MODE_RDWR, 
                MPI_INFO_NULL, &fh  );
  MPI_File_close(&fh);

  int k = xsize;

  // Write pgm header
  if (rank == 0) {
    FILE* file_stream = fopen(fname, "w");  // Open a FILE* stream
    if (file_stream != NULL) {
        int max_value = size - 1;
        fprintf(file_stream, "P5\n# generated by\n# Elena Rivaroli and Samuele D'Avenia\n%d %d\n%d\n", xsize, ysize, maxval);
        fclose(file_stream);
    } else {
        fprintf(stderr, "Failed to open the PGM file for writing.\n");
        MPI_Abort(MPI_COMM_WORLD, 1);
    }
  }
  MPI_Barrier(MPI_COMM_WORLD); // Wait for the header to have finished writing.
  
  // Open the file using MPI collective functions and write with each thread
  MPI_File_open(MPI_COMM_WORLD, fname, 
                MPI_MODE_APPEND | MPI_MODE_RDWR, 
                MPI_INFO_NULL, &fh  );
      
  // Decide where on file each MPI process should write.
  disp = (rank >= k % size) ? (rank * rows_initialize +k%size)* k *sizeof(unsigned char) : rank * rows_initialize * k *sizeof(unsigned char);
  
  MPI_File_seek(fh, disp, MPI_SEEK_CUR);
  MPI_File_write_all(fh, ptr, rows_initialize*k, MPI_UNSIGNED_CHAR, MPI_STATUS_IGNORE);

  MPI_File_close(&fh);
}

void initialize_parallel(int k, char *fname, int rank, int size, int rows_initialize){
  /*
  INPUT:
    - k: the size of the square where we want to execute GOL.
    - fname: the string containing the name of the pgm file where we want to store the image
  
  1. It generates a random matrix of size k x k. 
     Each MPI process is assigned a number of rows, and then OpenMP threads are used to randomly initialize different rows of the matrix
  
  2. It writes the random matrix to file called fname.
  */
  
  MPI_File fh;
  MPI_Offset disp;

  // Allocate the memory required for the rows of that processor.
  unsigned char* ptr = (unsigned char*)malloc(rows_initialize*k * sizeof(unsigned char)); // Allocate memory for the rows you have to generate.
  
  // In this parallel region the different threads generate random numbers on different sections of the matrix.
  //double Tstart_init = omp_get_wtime();
  double Tstart_generate;
  if(rank == 0) 
    Tstart_generate = omp_get_wtime();
  #pragma omp parallel
  {
    int my_id = omp_get_thread_num();
    
    unsigned int seed = clock();
    seed = seed * rank + my_id;

    #pragma omp for
    for (int i = 0; i < rows_initialize*k; i++){
        unsigned char random_num = (unsigned char) rand_r(&seed) % 2;
        ptr[i] = (random_num==1) ? 255 : 0;
    }

  }
  MPI_Barrier(MPI_COMM_WORLD);
  if(rank == 0){
    double Time_generate = omp_get_wtime() - Tstart_generate;
    printf("Generate time: %lf\n", Time_generate);
  }
  // Write to file.
  double Tstart_write;
  if(rank == 0) 
    Tstart_write = omp_get_wtime();
  write_pgm_parallel(ptr, 255, k, k, fname, rank, size, rows_initialize);
  MPI_Barrier(MPI_COMM_WORLD);
  if(rank == 0){
    double Time_write = omp_get_wtime() - Tstart_write;
    printf("Write time: %lf\n", Time_write);
  }

  free(ptr);
}

void read_pgm_parallel(unsigned char **ptr, int k, const char *image_name, int rank, int size, int rows_read){
  /*
  INPUT:
    - ptr: pointer to the memory location where the matrix will be stored
    - k: matrix size
    - image_name: name of file where the matrix is stored.
  
  Each process gets allocated an amount of memory which corresponds to the rows it has to evolve plus the one above and below
    which are needed for correct update in parallel.
  */
  int counter;
  if(rank==0){
    FILE* image_file; 
    image_file = fopen(image_name, "r");
    counter=0;

    int xsize;
    int ysize;
    int maxval;
    xsize = ysize = maxval = 0;

    char    MagicN[2]; // define a string of 2 elements
    char   *line = NULL; //define a pointer "line" to NULL
    size_t  t, n = 0;

    // get the Magic Number
    t = fscanf(image_file, "%2s%*c", MagicN ); // This one reads P5
    counter+=3;

    // skip all the comments
    t = getline( &line, &n, image_file); // Here we read all the lines starting with #, i.e. all the comments.
    counter+=t;
    while ( (t > 0) && (line[0]=='#') ){
      t = getline( &line, &n, image_file);
      counter+=t;
    }
    if (t > 0){
      t = sscanf(line, "%d%*c%d%*c%d%*c", &xsize, &ysize, &maxval);  // This one reads the number
      if ( t < 3 ){
        t = getline(&line,&n,image_file);
        counter+=t;
        sscanf(line, "%d%*c", &maxval);
      }
    }else{
      maxval = -1;         // this is the signal that there was an I/O error
      free( line );
      return;
    }
    free(line);
    fclose(image_file);
  }
  // Inserire broadcast
  MPI_Bcast(&counter, 1, MPI_INT, 0, MPI_COMM_WORLD);

  *ptr = (unsigned char*)malloc((rows_read+2)*k * sizeof(unsigned char));

  MPI_Offset disp;
  MPI_File   fh;
  MPI_File_open(  MPI_COMM_WORLD, image_name, 
                  MPI_MODE_RDONLY,
                  MPI_INFO_NULL, &fh  );

  // disp is the starting point for the rows each process has to evolve in the file
  disp = (rank >= k % size) ? (rank * rows_read + k % size) * k * sizeof(unsigned char) : rank * rows_read * k * sizeof(unsigned char);
  disp += counter;  
  
  // process 0 has to read the LAST row in the file
  //   all the others have to read the one before their "working" rows.
  if(rank==0)
    MPI_File_seek(fh, counter+k*k-k, MPI_SEEK_SET);
  else 
    MPI_File_seek(fh, disp-k, MPI_SEEK_SET);
  // Read into ptr the leftmost row
  MPI_File_read_all(fh, *ptr, k, MPI_UNSIGNED_CHAR, MPI_STATUS_IGNORE);

  // Now we read the working rows
  if(rank==0)
    MPI_File_seek(fh, counter, MPI_SEEK_SET);
  MPI_File_read_all(fh, (*ptr)+k, rows_read*k, MPI_UNSIGNED_CHAR, MPI_STATUS_IGNORE);

  // Finally we read the last row (symmetric case for the last processor as P0)
  if(rank==size-1)
    MPI_File_seek(fh, counter, MPI_SEEK_SET);
  MPI_File_read_all(fh, (*ptr)+k+rows_read*k, k, MPI_UNSIGNED_CHAR, MPI_STATUS_IGNORE);

  MPI_File_close(&fh);
  MPI_Barrier(MPI_COMM_WORLD);
}

